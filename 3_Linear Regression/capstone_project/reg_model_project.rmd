---
title: "Modeling and prediction for movies"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---

## Setup

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(dplyr)
library(statsr)
install.packages("GGally")
library(GGally)
```

### Load data

Make sure your data and R Markdown files are in the same directory. When loaded
your data file will be called `movies`. Delete this note when before you submit 
your work. 

```{r load-data}
load("movies.rdata")
```

* * *

## Part 1: Data

From the description of the data in the codebook we can see that "The data set is comprised of 651 randomly sampled movies produced and released before 2016", we can see that we have a random sample which implies that the data could be generalized to all movies released before 2014. However since the having such smaple could not be feasible let's see what the "movies.rdata" could tell us about the time range:

```{r}
summary(movies$thtr_rel_year)
```

As we can see from the sample statistics over the `thtr_rel_year` variable, the actual range is in between `1970` - `2014` so assuming that the random sample part of the description is `true` we would say that the data could be generalized to all movies that were released between 1970 to 2014.

This is an observational study and no random assignment was used so we cannot do any causal conclusions.

* * *

## Part 2: Research question

1. Do the movies that have good rating on `Rotten tomatoes` expected to also have a good rating in `IMDB`?

The question is of interest for the author since when he wants to see a movie, he have to read the rating on both `IMDB` and `Rotten Tomatoes` resources. Knowing a relation between rating of the two sites and by building a linear regression model, the author can potentially save a lot of time by using the model to predict the rating on the other resource instgead of reading the actual reviews.

* * *

## Part 3: Exploratory data analysis

Let's do an exploratory data analysis to see what the data we are working with.

### IMDB Rating

Let's explore the `imdb_rating` variable first.

```{r}
summary(movies$imdb_rating)
str(movies$imdb_rating)
```

As we can see, the `imdb_rating` is `numerical` that is rounded to one decimal place. While the `imdb` site has the `1-10` ranting range, the max rating value is actually for the data. Also since the `median` is very close to `mean` we expect the distribution not to be very skewed (but still skewed), since the `mean` is smaller than the `median` we expect to see a lower tail skew(the skewness pull the mean to the left).

Let's see the distribution:

```{r}
ggplot(movies, aes(x = imdb_rating)) + geom_histogram(binwidth = .5)
```

The histogramm confirms our asummptions - the distribution is slightly left skewed with majority of the observations in between `5.5` to `7.75` range which mean that the imdb users are not tend to give a movie a very small or very high mark with preference to the higher one.

### Audience Score

Let's do the same EDA sfor the `audience_score` variable.

```{r}
summary(movies$audience_score)
str(movies$audience_score)
```

The statistics show that this is numerical variable inbetween with range of `1` to `100` rounded to two decimal placesThe actual min is `11` and max is `97`, surprisingly looking at the computed statistics alone we can expect that the distribution is very similar to the one we had for the `imdb_rating` variable - `mean` is very close but still smaller to `median` which indicates a slight left skew. Let's build a histogram to visualize it:

```{r}
ggplot(movies, aes(x = audience_score)) + geom_histogram(binwidth = 10)
```

Our assumptions are about true, the distribution is not exact the same - it has much longer left tail thus more spread. The majority of the observation are in between of `25` to `90`, observations on the higher end are closer to the max score than we had for the `imdb_rating` variable.

### audience_score vs imdb_rating

Let's scatterplot the variables to see if there is any correlation trend between them:

```{r}
ggplot(movies, aes(x = audience_score, y = imdb_rating)) + geom_jitter()
```

On the plot we can see a strong positive about linear assosiation between the variables. We can also see a tiny curvature in the data, we will need to confirm the linearity condition when we will do the model diagnostics.

The correlation could be evaluated with a single number:

```{r}
cor(movies$imdb_rating, movies$audience_score)
```

The `cor` function supports our assumtions about strong assosiation between the variables.

* * *

## Part 4: Modeling


Let's see what variables could be helpful in linear regression modeling, for that we will create a matrix plot:


```{r}
# subset the `movies` data with the variables of interest
movies2 = data.frame(imdb_rating = movies$imdb_rating, best_actor_win = movies$best_actor_win, best_dir_win = movies$best_dir_win, best_actress_win = movies$best_actress_win, audience_score = movies$audience_score, runtime = movies$runtime, critics_score = movies$critics_score)

movies2 = movies2 %>% mutate(best_actor_win = ifelse(best_actor_win == 'yes', 1, 0))
movies2 = movies2 %>% mutate(best_actress_win = ifelse(best_actress_win == 'yes', 1, 0))
movies2 = movies2 %>% mutate(best_dir_win = ifelse(best_dir_win == 'yes', 1, 0))

ggpairs(movies2)
```

From the matrix plot above we can learn few lessons:

  1. `critics_score` have a large correlation value with the `imdb_rating` (0.765) but at the same time, the `critics_score` and `audience_score` are strongly assosiated as well. Thus using both variables for linear regression models will complicate model estimation.
  2. surprisingly, the `runtime` variable might be a significant predictor - the correlation with `imdb_rating` is 0.268 and
correlation with the `audience_score` variable ios fairly weak.
  3. the `best_actress_win`, `best_actor_win` and `best_dir_win` variables have very weak assosiation with the response variable thus might not be significant predictors.
  
We will use all of the variables above(`best_actress_win`, `best_actor_win`, `best_dir_win`, `audience_score`, `runtime` `critics_score`) for our full model to start with and see if our assumptions hold.

Let's build a linear regression model for predicting the `imdb_rating` score. Since we are interesed in model performance rather than finding which variables are significant predictor of the response variable, we will use `adjusted R^2` method. Whether we will do `forward selection` or `backward elimination` is not important for the study so we will pick the later.

```{r}
model = lm(imdb_rating ~ best_actor_win + best_dir_win + best_actress_win + audience_score + runtime + critics_score, data = movies2)

summary(model)
```

We have a large `adjusted R^2` value which evaluates the model performance. As we expected the `p-values` for the `best_actor_win`, `best_dir_win` and `best_actress_win` are much larger than the significance level of `5%` thus we will reject the null hypothesis concluding that thees data do not provide significant evidence that each of the variables are significant predictors of the response variable, given all else hold constant. Of course since we use the adjusted R^2 method, we will not use this insight for model optimization.


Next we will remove each variable one by one and see if absense of any of the variables gives use improvement in our `R^2` value:


```{r}
print('initial')
print(summary(model)$adj.r.squared)

m1 = lm(imdb_rating ~ best_dir_win + best_actress_win + 
    audience_score + runtime + critics_score, data = movies2)
print('best_actor_win')
print(summary(m1)$adj.r.squared)

m2 = lm(imdb_rating ~ best_actor_win + best_actress_win + 
    audience_score + runtime + critics_score, data = movies2)
print('best_dir_win')
print(summary(m2)$adj.r.squared)

m3 = lm(imdb_rating ~ best_actor_win + best_dir_win + 
    audience_score + runtime + critics_score, data = movies2)
print('best_actress_win')

m4 = lm(imdb_rating ~ best_actor_win + best_dir_win + best_actress_win + runtime + critics_score, data = movies2)
print('audience_score')
print(summary(m4)$adj.r.squared)

m5 = lm(imdb_rating ~ best_actor_win + best_dir_win + best_actress_win + 
    audience_score + critics_score, data = movies2)
print('runtime')
print(summary(m5)$adj.r.squared)

m6 = lm(imdb_rating ~ best_actor_win + best_dir_win + best_actress_win + 
    audience_score + runtime, data = movies2)
print('critics_score')
print(summary(m6)$adj.r.squared)
```

As we can see, fropping the `best_dir_win` will give us the most `adjusted R^2` gains. Next, taking the new improved `adjusted R^2` as the base value, we will need to repeat the same will variables that was not dropped, eliminating them, one by one, until the `adjusted R^2` will stop improving.


```{r}
m0 = lm(imdb_rating ~ best_actor_win + best_actress_win + 
    audience_score + runtime + critics_score, data = movies2)
print('initial')
print(summary(m0)$adj.r.squared)

m1 = lm(imdb_rating ~ best_actress_win + 
    audience_score + runtime + critics_score, data = movies2)
print('best_actor_win')
print(summary(m1)$adj.r.squared)

m2 = lm(imdb_rating ~ best_actor_win + 
    audience_score + runtime + critics_score, data = movies2)
print('best_actress_win')
print(summary(m2)$adj.r.squared)

m3 = lm(imdb_rating ~ best_actor_win + best_actress_win + 
     runtime + critics_score, data = movies2)
print('audience_score')
print(summary(m3)$adj.r.squared)

m4 = lm(imdb_rating ~ best_actor_win + best_actress_win + 
    audience_score + critics_score, data = movies2)
print('runtime')
print(summary(m4)$adj.r.squared)

m5 = lm(imdb_rating ~ best_actor_win + best_actress_win + 
    audience_score + runtime, data = movies2)
print('critics_score')
print(summary(m5)$adj.r.squared)
```

On this iteration, it makes sense to drop the `best_actor_win` variable a it gives us the biggest `adjusted R^2` gains.


```{r}
m0 = lm(imdb_rating ~ best_actress_win + 
    audience_score + runtime + critics_score, data = movies2)
print('initial')
print(summary(m0)$adj.r.squared)

m1 = lm(imdb_rating ~ audience_score + runtime + critics_score, data = movies2)
print('best_actress_win')
print(summary(m1)$adj.r.squared)

m2 = lm(imdb_rating ~ best_actress_win + 
     runtime + critics_score, data = movies2)
print('audience_score')
print(summary(m2)$adj.r.squared)

m3 = lm(imdb_rating ~ best_actress_win + 
    audience_score + critics_score, data = movies2)
print('runtime')
print(summary(m3)$adj.r.squared)

m4 = lm(imdb_rating ~ best_actress_win + 
    audience_score + runtime, data = movies2)
print('critics_score')
print(summary(m4)$adj.r.squared)
```

On this iteration, dropping the `best_actress_win` variable will give use the most gains in `adjusted R^2` value.

```{r}
m0 = lm(imdb_rating ~ audience_score + runtime + critics_score, data = movies2)
print('initial')
print(summary(m0)$adj.r.squared)

m1 = lm(imdb_rating ~ runtime + critics_score, data = movies2)
print('audience_score')
print(summary(m1)$adj.r.squared)

m2 = lm(imdb_rating ~ audience_score + critics_score, data = movies2)
print('runtime')
print(summary(m2)$adj.r.squared)

m3 = lm(imdb_rating ~ audience_score + runtime, data = movies2)
print('critics_score')
print(summary(m3)$adj.r.squared)
```

Dropping any of the variable will not give us any improvements in `adjusted R^2` value so we stop iterating, our model is ready.

```{r}
summary(m0)
```

We can see that the `F-statistic` for the model is `895.3` on 3 and 646 degrees of freedom, such a large critical F score gives us nearly zero `p-value`(<2.2e-16) that confirms model performace with such big `adjusted R^2` value (0.8052) which eventually means that at least one of the explanatory variable is a significant predictor of the response variable. At the same time, we can see that each of the variables have the tiny `p-values` which means that each of them is a significant predictors of the response(`imdb_rating`) variable given everything else hold constant.

###### Interpritation of slopes

  - *audince_score*: With additional 1 point increase of `audience_score`, we would expect increase in the `imdb_rating` score by `0.0340659` points on average, given everything else held constant.
  - *runtime*: With additional 1 point increase of `runtime`, we would expect increase in the `imdb_rating` score by
  `0.0056647` points on average, given everything else held constant.
  - *critics_score*: With additional 1 point increase of `critics_score`, we would expect increase in the `imdb_rating` score by `0.0114496` points on average, given everything else held constant.
  
###### Confidence Intervals and Their Interpritation

We can compute CIs for the given parameters, as we can remember CIs are allways `point estimate +- t*df * SE`.
We are given with the `point estimate` and `SE`, we also know the sample size so can compute the critical t-score `t*df`.

`df = n - k - 1` - because we loose 1 degree of freedom for each predictor
`df = 651 - 3 - 1 = 647`

Knowing the `df` value, we can compute the critical `t-score`, since the degrees of freedom value is large, we would expect the T-distibution to be close to normal distribution thus close to `1.96` for `95% CI`.

```{r}
abs(qt(0.025, df = 647))
```
As expected the critical score value is very close to the `1.96`.

Now we can compute the CIs:

**audience_score**

```{r}
ME = abs(qt(0.025, df = 647)) * 0.0013129
b = 0.0340659
print(b - ME)
print(b + ME)
```

We are `95%` sure, that for each additional point in `audience_score`, the response variable, `imdb_rating` expected to be by 0.03148784 larger to 0.03664396 larger, given all esle held constant. The result agreed with the `p-value` as the `CI` does not contain the `null-value` (0) which means we would reject the null-hypothesis given the CI alone.

**runtime**

```{r}
ME = abs(qt(0.025, df = 647)) * 0.0009848
b = 0.0056647
print(b - ME)
print(b + ME)
```

We are `95%` sure, that for each additional point in `audience_score`, the response variable, `imdb_rating` expected to be by 0.00373091 larger to 0.00759849 larger, given all esle held constant.  The result agreed with the `p-value` as the `CI` does not contain the `null-value` (0) which means we would reject the null-hypothesis given the CI alone.

**critics_score**

```{r}
ME = abs(qt(0.025, df = 647)) * 0.0009336
b = 0.0114496
print(b - ME)
print(b + ME)
```

We are `95%` sure, that for each additional point in `audience_score`, the response variable, `imdb_rating` expected to be by 0.009616348 larger to 0.01328285 larger, given all esle held constant.  The result agreed with the `p-value` as the `CI` does not contain the `null-value` (0) which means we would reject the null-hypothesis given the CI alone.


### Model Diagnostics

We need to perform model diagnostics to see if each of the variable conforms with the conditions needed for linear regression modeling.

#### Linearity

##### audience_score

> The relationship between `explanatory` and `response` variable should be `linear`.

For checking the linearity condition, we can fit a line to the scatterplot along with a residuals plot:

```{r}
ggplot(movies, aes(x = imdb_rating, y = audience_score)) + geom_jitter() + geom_smooth(method = "lm")

plot(m0$residuals ~ movies$audience_score) # linear assosiation = random scatter
```

We expect to see exactly random scatter around `0` to confirm the `linearity` condition, we can witness that the condition  holds overall, there are some outliers at the bottom of the page but that should be ok as there are few of them.

##### runtime

For checking the linearity condition, we can fit a line to the scatterplot along with a residuals plot:

```{r}
ggplot(movies, aes(x = imdb_rating, y = runtime)) + geom_jitter() + geom_smooth(method = "lm")

plot(m0$residuals ~ movies$runtime) # linear assosiation = random scatter
```

We can confirm that the condition overall holds, the majority of observations clustered in a blob with the center in `0`, with some of the outliers at the bottom and at right right of the blob.

##### critics_score

For checking the linearity condition, we can fit a line to the scatterplot along with a residuals plot:

```{r}
ggplot(movies, aes(x = imdb_rating, y = critics_score)) + geom_jitter() + geom_smooth(method = "lm")

plot(m0$residuals ~ movies$critics_score) # linear assosiation = random scatter
```

We can see that the condition holds overall with, there are some observations in left bottom corner that must be clearly stated in reservations of the model.



##### Constant Variability

Let's check another codition for linear regression modeling - the `constant variability`, for that we will check plots of residuals vs their fit values as well as `absolute` value of the residuals vs their fit. If the `constant variability` condition holds we do not expect to see a fan shape on the plots:

```{r}
plot(m0$residuals ~ m0$fitted)
plot(abs(m0$residuals) ~ m0$fit)
```

On the first and the second plot we cannot see a clear trend in the data, the seond plot does not contain any triangle which means there is no fan shape can be observed.

##### Nearly Normal Residuals

If the nearly normal residuals condition holds, we would see a nearly-normal unimodal symmetric distribution on residuals histogram, also we can check the `theoretical quantiles` to spot any deviations from the normality line:

```{r}
hist(md$residuals)

qqnorm(md$residuals); qqline(md$residuals)
```

We see a slight right skew in the histogram, but with such a big sample it might be totally fine. Looking at the theoretical quantiles plot we see few outliers at the higher end, while the deviation might be strong these are just few observations so we can conclude that the condition holds with small reservations.

##### Independence

While we have a random sample which speaks to independence of the residuals, let's plot the residuals to try to spot any pattern which will mean that we have time series observations(non-idependence):

```{r}
plot(md$residuals)
```

The residuals plot looks totally fine, the data is randomly scattered around 0 without any obvious pattern - the condition of independence holds.


## Part 5: Prediction

NOTE: Insert code chunks as needed by clicking on the "Insert a new code chunk" 
button above. Make sure that your code is visible in the project you submit. 
Delete this note when before you submit your work.

* * *

## Part 6: Conclusion

